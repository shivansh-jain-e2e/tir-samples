{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adb2e1e-da6d-4cbd-b041-a6bc6ac1b618",
   "metadata": {},
   "source": [
    "# Install and Start VLLM Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c903b0-61d1-4a45-9314-25c015766272",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "1. Go to TIR Dashboard >> Nodes and Create a new Node with H100 Plan \n",
    "\n",
    "2. Once Notebook is in running state, open the jupyter labs \n",
    "\n",
    "3. Launch a **new terminal** in jupyter labs and run the following commands:\n",
    "\n",
    "#### Install virtual env\n",
    "\n",
    "```\n",
    "sudo apt update\n",
    "apt install python3.10-venv\n",
    "```\n",
    "\n",
    "#### Create virtual env\n",
    "\n",
    "```\n",
    "python -m venv vllm_env\n",
    "```\n",
    "\n",
    "#### Activate virtual env\n",
    "\n",
    "```\n",
    "source vllm/bin/activate\n",
    "```\n",
    "\n",
    "#### Install vllm\n",
    "\n",
    "```\n",
    "pip install vllm\n",
    "```\n",
    "\n",
    "#### Start VLLM server with llama3 \n",
    "\n",
    "```\n",
    "vllm serve meta-llama/Meta-Llama-3-8B-Instruct\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71810ee1-19ec-4757-bb31-9213b6bb52d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test the endpoint\n",
    "Once the server is up, launch a new notebook (from jupyter labs launch screen) and run the following test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f82fc6-2343-42e2-acbd-457177031921",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2088297-1ea7-4ee0-93c6-813e6c0d731e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff953146-e53e-43ec-9241-b026df1de720",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install aiohttp\n",
    "!pip install openpyxl\n",
    "!pip install xlrd\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37c0c4-6f23-4d5f-b2b2-dcd5e27e4ac6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports and basic constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b2032c-bed1-4411-835b-bee1a352fc9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "import requests\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "ssl.create_default_context().cert_store_stats()\n",
    "\n",
    "API_TOKEN = \"sdfdsf\"\n",
    "\n",
    "headers = {'Authorization': f'Bearer {API_TOKEN}', 'Content-Type': 'application/json'}\n",
    "endpoint_url = 'http://localhost:8000/v1/chat/completions'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b2ea5-4933-437c-83a4-62c0f9af168a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset file path and other generation variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e1b4defa-2f2b-41b5-84b3-ec9804aa6b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of outut tokens to benchmark\n",
    "output_tokens = 256\n",
    "# Number of queries per thread, higher gives more accurate results. max number equels to column. length of dataset\n",
    "num_queries_per_thread = 20\n",
    "# dataset Path and column name\n",
    "file_path = 'prompts.xlsx'  # Change this to the path of your file\n",
    "column_name = 'prompt_text'  # Change this to the column you want to extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb240c7-4eee-4adb-8681-8bc08b62ef4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read From Dataset And Convert Request Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c22bcf-8993-4883-87c9-0fd3da1fe7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_column(file_path, column_name):\n",
    "    # Determine the file extension\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    # Load the file into a DataFrame based on its extension\n",
    "    if file_extension == '.xlsx':\n",
    "        df = pd.read_excel(file_path)\n",
    "    elif file_extension == '.xls':\n",
    "        df = pd.read_excel(file_path, engine='xlrd')\n",
    "    elif file_extension == '.csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only .xlsx, .xls, and .csv are supported.\")\n",
    "    \n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the file.\")\n",
    "    \n",
    "    # Extract the column\n",
    "    column_data = df[column_name]\n",
    "\n",
    "    return column_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b084da0-cf82-4da6-b3fd-142bdb3b61f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Edit System and User Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241245f3-2acf-42b5-8635-0512c83b16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit system prompt as per your use case\n",
    "system_prompt = \"You are an expert in ....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac3933b-a695-441d-8a52-f0b582c52b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# edit your user prompt here.  \n",
    "# The column value from excel sheet (prompts.xls) will be appear here as user_input\n",
    "def generate_prompt(user_input): \n",
    "    return f\"\"\"Extract the information from this input in JSON format: {user_input}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4829fa1-bfab-4fb6-b181-a077feb8cbed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07ebfc3c-1b34-4038-bb39-b06fb9024e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_request_data(file_path, column_name, num_requests, out_tokens, system_prompt):\n",
    "    target_col = read_column(file_path, column_name)\n",
    "\n",
    "    prompt_list = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        # commenting this for now to test blank values. \n",
    "        # if not target_col[i] or target_col[i] == \"Ad\" or str(target_col[i]) == \"nan\" :\n",
    "        #     continue\n",
    "        \n",
    "        prompt = {\n",
    "            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            \"messages\": [\n",
    "                 {\"role\": \"system\", \"content\": system_prompt},\n",
    "                 {\"role\": \"user\", \"content\": generate_prompt(target_col[i])}\n",
    "                ],\n",
    "            'temperature': 0.0,\n",
    "            'max_tokens': out_tokens,\n",
    "        }\n",
    "        prompt_list.append(prompt)\n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95bb2da-5f7d-4a31-98d0-a8a82eb629dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the request generator function \n",
    "get_request_data(file_path, column_name, 10, output_tokens, system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a17eb-8bae-4b74-9687-6a1bd5d44b66",
   "metadata": {
    "tags": []
   },
   "source": [
    "# For Warmup and validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f4b87-d007-4667-bb6c-3d4874b6731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends an inital set of warm up requests and validates that we are sending the correct number of input tokens.\n",
    "def warm_up_and_validate(out_tokens=256, warm_up_requests=10):\n",
    "    input_data = get_request_data(file_path, column_name, warm_up_requests, out_tokens, system_prompt)\n",
    "    print(\"total inputs selected:\", len(input_data))\n",
    "    loop_length = warm_up_requests if warm_up_requests < len(input_data) else len(input_data)\n",
    "    for i in range(loop_length):\n",
    "        input_json = json.dumps(input_data[i])\n",
    "        # print(f\"input: {input_json}\")\n",
    "        req = requests.Request('POST', endpoint_url, headers=headers, data=input_json)\n",
    "        prepped = req.prepare()\n",
    "        session = requests.Session()\n",
    "        resp = session.send(prepped)\n",
    "        result = json.loads(resp.text)\n",
    "        # print(f\"result: {result}\")\n",
    "        print(result['usage']['completion_tokens'])\n",
    "        print(result['usage']['prompt_tokens'])\n",
    "\n",
    "warm_up_and_validate(output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb2440-231f-4a1c-a632-79a8ab2660f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Single Worker Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b01f21f3-6904-4025-a4da-a7649d89cbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latencies = []\n",
    "\n",
    "async def worker(index, num_requests, out_tokens=256):\n",
    "    input_data = get_request_data(file_path, column_name, num_requests, out_tokens, system_prompt)\n",
    "    await asyncio.sleep(0.1 * index)  # Offset the start time of each worker\n",
    "    loop_length = num_requests if num_requests < len(input_data) else len(input_data)\n",
    "\n",
    "    for i in range(loop_length):\n",
    "        input_json = json.dumps(input_data[i])\n",
    "        request_start_time = time.time()\n",
    "        success = False\n",
    "        retries = 0\n",
    "\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                timeout = aiohttp.ClientTimeout(total=3 * 3600)\n",
    "                connector = aiohttp.TCPConnector(ssl=False)\n",
    "                async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "                    async with session.post(endpoint_url, headers=headers, data=input_json) as response:\n",
    "                        response_text = await response.text()\n",
    "                        # print(f\"Response {i} from worker {index}: {response_text}\")\n",
    "\n",
    "                        if response.ok:\n",
    "                            success = True\n",
    "                            \n",
    "                            latency = time.time() - request_start_time\n",
    "                            result = json.loads(response_text)\n",
    "                            latencies.append((result['usage']['prompt_tokens'],\n",
    "                                              result['usage']['completion_tokens'], latency))\n",
    "                        else:\n",
    "                            print(f\"Error {response.status}: {response_text}\")\n",
    "                            retries += 1\n",
    "                            await asyncio.sleep(1)  # Backoff before retrying\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "                retries += 1\n",
    "                await asyncio.sleep(1)  # Backoff before retrying\n",
    "\n",
    "async def single_benchmark(num_requests_per_worker, num_workers, out_tokens=256):\n",
    "    tasks = []\n",
    "    for i in range(num_workers):\n",
    "        task = asyncio.create_task(worker(i, num_requests_per_worker, out_tokens))\n",
    "        tasks.append(task)\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb4f8c-8ebf-4ce7-bb2f-bc32b5d18572",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Execute With Parallel Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2bbc4b65-6669-4819-9619-b8b8fd971c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This runs the benchmark with 1, n/2 and n output tokens to enable deriving time to first token (from 1 output token)\n",
    "# and the time per token by looking at the difference in latency between 64 and 128 output tokens.\n",
    "async def benchmark(parallel_queries=1, out_tokens=256, num_tries=5):\n",
    "    # store statistics about the number of input/outpu and the latency for each setup.\n",
    "    avg_num_input_tokens = [0, 0, 0]\n",
    "    avg_num_output_tokens = [0, 0, 0]\n",
    "    median_latency = [0, 0, 0]\n",
    "    print(f\"Parallel queries {parallel_queries}\")\n",
    "    for i, out_tokens in enumerate([1, out_tokens/2, out_tokens]):\n",
    "        # Clear the latencies array so that we get fresh statistics.\n",
    "        latencies.clear()\n",
    "        await single_benchmark(num_tries, parallel_queries, out_tokens)\n",
    "        # Compute the median latency and the mean number of tokens.\n",
    "        avg_num_input_tokens[i] = statistics.mean([inp for inp, _, _ in latencies])\n",
    "        avg_num_output_tokens[i] = statistics.mean([outp for _, outp, _ in latencies])\n",
    "        median_latency[i] = statistics.median([latency for _, _, latency in latencies])\n",
    "        tokens_per_sec = (avg_num_input_tokens[i]+avg_num_output_tokens[i])*parallel_queries/median_latency[i]\n",
    "        print(f'Avg. Input Tokens {avg_num_input_tokens[i]}, Avg. Output tokens {avg_num_output_tokens[i]}, median latency (s): {round(median_latency[i], 2)}, tokens per second {round(tokens_per_sec, 1)}')\n",
    "    # We use difference in the time between out_tokens/2 and out_tokens to generate find the time per output token\n",
    "    # these are stored in median_latency[1] and median_latency[2] respectively\n",
    "    # We use the time to generate just 1 token to get the time to first token, this is stored in median_latency[0]\n",
    "    output_token_time = (median_latency[2] - median_latency[1])*1000/(avg_num_output_tokens[2]-avg_num_output_tokens[1])\n",
    "    print(f'Time to first token (s): {round(median_latency[0],2)}, Time per output token (ms) {round(output_token_time,2)}')\n",
    "    data.append([median_latency[2],\n",
    "               (avg_num_input_tokens[2]+avg_num_output_tokens[2])*parallel_queries/median_latency[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90740c6d-f4e0-4234-b72e-b2af72e84ac6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark And Draw Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e2a14-d200-4ab6-84c7-30ee3a211005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run until the throughput of the model is no longer increasing by 10%.\n",
    "data = []\n",
    "for parallel_queries in [1, 2, 4, 8]:\n",
    "    request_per_worker = 100\n",
    "    await benchmark(parallel_queries, output_tokens, request_per_worker)\n",
    "    # Break if the throughput doesn't increase by more than 10%\n",
    "    if len(data) > 1 and (data[-1][1] - data[-2][1])/data[-2][1] < 0.1:\n",
    "        break\n",
    "\n",
    "# Plot the latency vs throughput curve\n",
    "plt.xlabel(\"Latency (s)\")\n",
    "plt.ylabel(\"Throughput (tok/s)\")\n",
    "line = plt.plot([x[0] for x in data], [x[1] for x in data], marker='o')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
