{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a70427-131f-4dd1-b91d-a6abbab11f38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensor RT for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084043ba-1bcc-4be5-98f2-cdc3bd8a7e04",
   "metadata": {},
   "source": [
    "This TIR Image comes with all pre-packaged with \n",
    "- tensor-rt LLM scripts (/app/scripts)\n",
    "- triton server with tensor-rt backend (/app/scripts/launch_triton_server.py)\n",
    "- tensor-rt LLM examples (/app/tensorrt_llm/examples/llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2b4f0-dbc8-40a1-ac63-6bdbd6c50b00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download model weights from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65af38c-c454-4b92-8622-3eefe2059918",
   "metadata": {},
   "source": [
    "Before we proceed, lets upgrade huggingface hub and download the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d643e-e9cc-4e38-aa3c-718e85413077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68979b99-eb05-48b0-abac-5d6e9453e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download sarvamai/OpenHathi-7B-Hi-v0.1-Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1eaf1-b27e-4eb1-a4d6-31aede808ee7",
   "metadata": {},
   "source": [
    "The model will be downloaded to $HOME/.cache folder. Run the following to get actual directory in which base model (OpenHathi) will be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7739820c-06ed-461a-bed5-42738a802edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2cb5807b852028defa07c56c96a7ff5c11f8df0e\n"
     ]
    }
   ],
   "source": [
    "!ls  $HOME/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725208b-4edd-43d3-883f-e49fe50401e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Assign the directory name above to BASE_MODEL_PATH (see the last bit in the assignment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50655e63-7498-4ec6-a8f8-8bd3fe132e92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_MODEL_PATH=/home/jovyan/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/snapshots/2cb5807b852028defa07c56c96a7ff5c11f8df0e\n"
     ]
    }
   ],
   "source": [
    "%env BASE_MODEL_PATH=/home/jovyan/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/snapshots/2cb5807b852028defa07c56c96a7ff5c11f8df0e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3657dbf3-2cb0-4164-a78c-72c2dfecbca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/snapshots/2cb5807b852028defa07c56c96a7ff5c11f8df0e\n"
     ]
    }
   ],
   "source": [
    "!echo $BASE_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfc19a-2062-4a61-addc-90a11ac2ed69",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build TensorRT LLM engine for Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f989a8-c5ec-49c2-bd60-8aa78c37467a",
   "metadata": {},
   "source": [
    "TensorRT LLM requires the huggingface weights to be first converted into a format that tensorRT library can understand. The command below will generate checkpoint files in /home/jovyan/ckpt folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad9af6-b3e2-4eab-a178-029b1e682152",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /app/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "--model_dir /home/jovyan/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/snapshots/2cb5807b852028defa07c56c96a7ff5c11f8df0e  \\\n",
    "-- --output_dir /home/jovyan/ckpt --dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa59001-08f4-45e3-af08-0de1f564e98d",
   "metadata": {},
   "source": [
    "Now we can create an engine with the checkpoint directory (above). here model will create an optimised engine for a specific GPU architecture. hence, you can only run this engine on same make of the GPU card. For e.g. an engine created on A100 can only be run (inference) on A100 cards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05178669-6031-4445-b216-59453059f8f2",
   "metadata": {},
   "source": [
    "The following command creates a plain and simple tensorRT engine. No Lora, neither quantization is considered at this point. We will cover those topics in later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f60288-17f4-4a37-85a4-ba329a2ec6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtllm-build --checkpoint_dir /home/jovyan/ckpt \\\n",
    "            --output_dir /home/jovyan/base-engine \\\n",
    "            --gemm_plugin float16 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de71080-3a2c-4480-be66-750a65cad14a",
   "metadata": {},
   "source": [
    "If the above command runs successfully, you will find engine files created in /home/jovyan/base_engine. The next step is to test the engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ca2f7-66a3-4fd3-bd4a-1e5ab90cc22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /app/tensorrt_llm/examples/run.py --run_profiling --engine_dir \"/home/jovyan/engine-lora\" \\\n",
    "              --max_output_len 125 \\\n",
    "              --tokenizer_dir \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\" \\\n",
    "              --input_text \"मैं एक अच्छा हाथी हूँ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ace34-1294-4d34-9220-6a11de830e5a",
   "metadata": {},
   "source": [
    "Note that a specific tokenizer is being passed in the run command above. This script will run inference on given engine, and use the specified tokenizer. If you have custom tokenizer, you can also mention directory where the tokenizer is available. \n",
    "The above command will also show the latency for batch size 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86893005-f096-41c4-86fc-2f254d913b35",
   "metadata": {},
   "source": [
    "### Build TensorRT LLM engine for your custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4113f-14af-4b30-8369-88a872dc3331",
   "metadata": {},
   "source": [
    "If you have a fully-fine tuned model then the steps would be similar to the base model (shown above) with exception that the checkpoint will be created with directory path where you custom model is stored. Do note, here we are still referring to full model and not lora/qlora yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06744e6a-7662-4889-9d5d-6bdaf4a75008",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /app/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "--model_dir /home/jovyan/custom-model-dir \\\n",
    "-- --output_dir /home/jovyan/ckpt --dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938f369-a329-404b-8934-8875dc86586d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use TensorRT LLM engine with Lora weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5eacc6-0fef-4cd2-9cef-760e9e9faf2e",
   "metadata": {},
   "source": [
    "When working with LORA we have two options:\n",
    "- merge base model with lora weights (adapters) and create engine for merged model\n",
    "- create engine with base model and pass lora weights (adapter) during run-time \n",
    "\n",
    "As it turns out, the second option offers most flexibility as well as quality.  hence, we will follow this option in the method below. \n",
    "\n",
    "Here we will still work with full model (base model) in FP16. And the LORA weights are also FP16. In later section, we will look at even more optimal configuration of (base model in INT4 + LORA weights FP16). \n",
    "\n",
    "The advantage of quantization will be lower gpu memory requirement during inference time. It can however impact performance, so it is recommended to test model for quality. Tensor RT also offers an easy way to test this out. More on this in later sections. \n",
    "\n",
    "Lets see how to build engine with LORA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055135ac-316a-4b1e-9324-8d4b28a075ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the lora_dir parameter to wherever your lora weights are stored. The engine will be created in engine-lora folder\n",
    "\n",
    "trtllm-build --checkpoint_dir /home/jovyan/ckpt \\\n",
    "            --output_dir /home/jovyan/engine-lora \\\n",
    "            --gemm_plugin float16 \\\n",
    "            --lora_plugin float16 \\\n",
    "            --lora_dir  \"/home/jovyan/trained-lora\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90350139-9050-4a26-bf8d-ce03f5b6aa2e",
   "metadata": {},
   "source": [
    "Test the engine with lora weights and tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf5203-38dc-470a-b346-98c81e1c38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /app/tensorrt_llm/examples/run.py --engine_dir \"home/jovyan/engine-lora\" \\\n",
    "              --max_output_len 125 \\\n",
    "              --tokenizer_dir \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\" \\\n",
    "              --input_text \"मैं एक अच्छा हाथी हूँ\" \\\n",
    "              --lora_dir \"/home/jovyan/trained-lora/\" \\\n",
    "              --lora_task_uids 0 \\\n",
    "              --no_add_special_tokens \\\n",
    "              --use_py_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb271e-e0ee-4952-9857-e6c58b71b4e5",
   "metadata": {},
   "source": [
    "To test the same engine without lora just pass lora_task_uids -1. This will offer good idea on how model performs with and without lora. You can also compare the quality of result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753b5c3-1891-48a1-a440-319bab7c4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /app/tensorrt_llm/examples/run.py --engine_dir \"home/jovyan/engine-lora\" \\\n",
    "              --max_output_len 125 \\\n",
    "              --tokenizer_dir \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\" \\\n",
    "              --input_text \"मैं एक अच्छा हाथी हूँ\"  \\\n",
    "              --lora_dir \"/home/jovyan/trained-lora/\" \\\n",
    "              --lora_task_uids -1 \\\n",
    "              --no_add_special_tokens \\\n",
    "              --use_py_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00cc3a-0130-47b4-ba0d-10e8e5a2bd58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use TensorRT LLM engine with INT4 quantization - Lora weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073473ab-2767-45bd-b2a2-9f9b5ddf63c2",
   "metadata": {},
   "source": [
    "The huggingface method of quantization with bitsandbytes does not work out of the box with Tensor RT. here we need to run a post quantization script to generate INT4 quantized version (or any variant) of base model.\n",
    "\n",
    "Note: you may change model_dir if you are storing base model or custom model in other directories. This model is expected to be full model and not lora (adapter) weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c9983-7526-4eb5-8c46-029b0a8866ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python /app/tensorrt_llm/examples/quantization/quantize.py --model_dir /home/jovyan/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/snapshots/2cb5807b852028defa07c56c96a7ff5c11f8df0e  \\\n",
    "                                   --output_dir /home/jovyan/int4-weights \\\n",
    "                                   --dtype float16 \\\n",
    "                                   --qformat int4  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33eaa2-bbd0-4812-b382-09e559964dd3",
   "metadata": {},
   "source": [
    "We can now use the quantised version of base model (from /home/int4-weights) to create tensorrt-engine with lora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b03d97-96f1-4217-bdc6-db633e987194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the lora_dir parameter to wherever your lora weights are stored. The engine will be created in engine-lora folder\n",
    "\n",
    "!trtllm-build --checkpoint_dir /home/jovyan/ckpt \\\n",
    "            --output_dir /home/jovyan/engine-int4 \\\n",
    "            --gemm_plugin float16 \\\n",
    "            --lora_plugin float16 \\\n",
    "            --lora_dir  \"/home/jovyan/trained-lora\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd500e-313c-413e-b656-5f11cf1435a0",
   "metadata": {},
   "source": [
    "Note: The int4 quantization is available in latest tensorRT (v.0.11.0). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c0183-8df8-467a-9175-80b2ee074055",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once we have an engine (int4 quantised) we can now run the test again with lora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140408dc-01f1-465b-af97-ca34b004aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /app/tensorrt_llm/examples/run.py --engine_dir \"home/jovyan/engine-int4\" \\\n",
    "              --max_output_len 125 \\\n",
    "              --tokenizer_dir \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\" \\\n",
    "              --input_text \"मैं एक अच्छा हाथी हूँ\" \\\n",
    "              --lora_dir \"/home/jovyan/trained-lora/\" \\\n",
    "              --lora_task_uids 0 \\\n",
    "              --no_add_special_tokens \\\n",
    "              --use_py_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2bafc9-4132-43b4-b127-0ac1c05d42a1",
   "metadata": {},
   "source": [
    "### References\n",
    "You can find more references and methods for further optimization here [https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llama/README.md]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
